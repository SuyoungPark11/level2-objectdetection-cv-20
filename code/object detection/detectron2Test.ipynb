{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='my_dataset_val',\n",
       "          json_file='../../dataset/test.json',\n",
       "          image_root='../../dataset',\n",
       "          evaluator_type='coco',\n",
       "          version='1.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer, hooks\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec, build_model\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.utils.comm import synchronize\n",
    "\n",
    "import timm\n",
    "\n",
    "# 추가적으로 필요한 import\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.structures import Instances\n",
    "from detectron2.evaluation import DatasetEvaluators\n",
    "from detectron2.utils.env import seed_all_rng\n",
    "\n",
    "# inference_context를 위한 import\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"detectron2\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"detectron2\").disabled = True\n",
    "\n",
    "@BACKBONE_REGISTRY.register()\n",
    "class TimmBackbone(Backbone):\n",
    "    def __init__(self, cfg, input_shape):\n",
    "        super().__init__()\n",
    "        model_name = cfg.MODEL.BACKBONE.TIMM_MODEL\n",
    "        self.model = timm.create_model(model_name, features_only=True, pretrained=True)\n",
    "        feature_info = self.model.feature_info.get_dicts(keys=['num_chs', 'reduction'])\n",
    "        \n",
    "        self.out_channels = 256\n",
    "        self.convs = nn.ModuleDict()\n",
    "        for i, info in enumerate(feature_info):\n",
    "            self.convs[f\"p{i+2}\"] = nn.Conv2d(info['num_chs'], self.out_channels, kernel_size=1)\n",
    "        \n",
    "        # P6, P7 레벨 추가 (RetinaNet용)\n",
    "        self.p6 = nn.Conv2d(feature_info[-1]['num_chs'], self.out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.p7 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self._out_features = [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]\n",
    "        self._out_feature_channels = {name: self.out_channels for name in self._out_features}\n",
    "        self._out_feature_strides = {f\"p{i+2}\": info['reduction'] for i, info in enumerate(feature_info)}\n",
    "        self._out_feature_strides[\"p6\"] = self._out_feature_strides[\"p5\"] * 2\n",
    "        self._out_feature_strides[\"p7\"] = self._out_feature_strides[\"p6\"] * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        out = {f\"p{i+2}\": self.convs[f\"p{i+2}\"](feature) for i, feature in enumerate(features)}\n",
    "        p6 = self.p6(features[-1])\n",
    "        p7 = self.p7(F.relu(p6))\n",
    "        out[\"p6\"] = p6\n",
    "        out[\"p7\"] = p7\n",
    "        return out\n",
    "\n",
    "    def output_shape(self):\n",
    "        return {\n",
    "            name: ShapeSpec(\n",
    "                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n",
    "            )\n",
    "            for name in self._out_features\n",
    "        }\n",
    "\n",
    "    \n",
    "image_dir = '../../dataset'\n",
    "json_file_path = '../../dataset/train.json'\n",
    "\n",
    "# 데이터셋 등록 (COCO 형식 가정)\n",
    "register_coco_instances(\"my_dataset_train\", {}, image_dir + \"/train.json\", image_dir)\n",
    "register_coco_instances(\"my_dataset_val\", {}, image_dir + \"/test.json\", image_dir)\n",
    "# 'version' 속성 추가\n",
    "MetadataCatalog.get(\"my_dataset_train\").set(version=\"1.0\")\n",
    "MetadataCatalog.get(\"my_dataset_val\").set(version=\"1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(model_type):\n",
    "    cfg = get_cfg()\n",
    "    if model_type == \"Cascade R-CNN\":\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        cfg.MODEL.FPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "    elif model_type == \"RetinaNet\":\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "        cfg.MODEL.FPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]\n",
    "    else:  # FPN-based Faster R-CNN\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        cfg.MODEL.FPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "    cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
    "\n",
    "    cfg.MODEL.BACKBONE.NAME = \"TimmBackbone\"\n",
    "    cfg.MODEL.BACKBONE.TIMM_MODEL = \"resnet50\"    \n",
    "    \n",
    "    cfg.MODEL.FPN.OUT_CHANNELS = 256     \n",
    "\n",
    "    cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32], [64], [128], [256], [512]]\n",
    "    \n",
    "    cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]\n",
    "    cfg.MODEL.RPN.IN_FEATURES = cfg.MODEL.FPN.IN_FEATURES\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = \"\"\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = (1024,)\n",
    "    cfg.INPUT.MAX_SIZE_TRAIN = 1024\n",
    "    cfg.INPUT.MIN_SIZE_TEST = 1024\n",
    "    cfg.INPUT.MAX_SIZE_TEST = 1024\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 128\n",
    "    cfg.SOLVER.BASE_LR = 0.00025\n",
    "    # epoch 설정\n",
    "    dataset_size = 4883\n",
    "    num_epochs = 3  # 원하는 epoch 수\n",
    "    iterations_per_epoch = dataset_size // cfg.SOLVER.IMS_PER_BATCH\n",
    "    cfg.SOLVER.MAX_ITER = iterations_per_epoch * num_epochs\n",
    "    cfg.TEST.EVAL_PERIOD = num_epochs\n",
    "    # cuDNN 벤치마크 비활성화\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "\n",
    "    # 학습률 스케줄 조정 (선택사항)\n",
    "    #cfg.SOLVER.STEPS = (iterations_per_epoch * 30, iterations_per_epoch * 40) \n",
    "\n",
    "    cfg.MODEL.MASK_ON = False\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/11 01:15:18 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): TimmBackbone(\n",
      "    (model): FeatureListNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (convs): ModuleDict(\n",
      "      (p2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (p3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (p4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (p5): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (p6): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.5, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/11 01:15:18 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/11 01:15:18 d2.data.datasets.coco]: \u001b[0mLoaded 4883 images in COCO format from ../../dataset/train.json\n",
      "\u001b[32m[10/11 01:15:18 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 4883 images left.\n",
      "\u001b[32m[10/11 01:15:18 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|\n",
      "| General trash | 3966         |    Paper    | 6352         | Paper pack | 897          |\n",
      "|     Metal     | 936          |    Glass    | 982          |  Plastic   | 2943         |\n",
      "|   Styrofoam   | 1263         | Plastic bag | 5178         |  Battery   | 159          |\n",
      "|   Clothing    | 468          |             |              |            |              |\n",
      "|     total     | 23144        |             |              |            |              |\u001b[0m\n",
      "\u001b[32m[10/11 01:15:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(1024,), max_size=1024, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[10/11 01:15:18 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[10/11 01:15:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/11 01:15:18 d2.data.common]: \u001b[0mSerializing 4883 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/11 01:15:18 d2.data.common]: \u001b[0mSerialized dataset takes 2.19 MiB\n",
      "\u001b[32m[10/11 01:15:18 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=128\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/11 01:15:18 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/11 01:15:19 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/11 01:15:19 d2.data.datasets.coco]: \u001b[0mLoaded 4883 images in COCO format from ../../dataset/train.json\n",
      "\u001b[32m[10/11 01:15:19 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from  ...\n",
      "Dataset version: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/11 01:15:19 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[10/11 01:15:23 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 155, in train\n",
      "    self.run_step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py\", line 496, in run_step\n",
      "    self._trainer.run_step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 310, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py\", line 158, in forward\n",
      "    features = self.backbone(images.tensor)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_73013/1799309134.py\", line 63, in forward\n",
      "    features = self.model(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/timm/models/_features.py\", line 346, in forward\n",
      "    return list(self._collect(x).values())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/timm/models/_features.py\", line 300, in _collect\n",
      "    x = module(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2438, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n",
      "\u001b[32m[10/11 01:15:23 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:03 (0:00:00 on hooks)\n",
      "\u001b[32m[10/11 01:15:23 d2.utils.events]: \u001b[0m iter: 0       lr: N/A  max_mem: 18093M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/114 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/11 01:15:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/11 01:15:24 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[10/11 01:15:24 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'my_dataset_train_eval' to COCO format ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/11 01:15:24 d2.data.datasets.coco]: \u001b[0mUsing previously cached COCO format annotations at './output_FPN-based Faster R-CNN/inference/my_dataset_train_eval_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "Building test loader for dataset: my_dataset_train_eval\n",
      "\u001b[32m[10/11 01:15:24 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|\n",
      "| General trash | 745          |    Paper    | 1663         | Paper pack | 162          |\n",
      "|     Metal     | 220          |    Glass    | 248          |  Plastic   | 692          |\n",
      "|   Styrofoam   | 280          | Plastic bag | 1103         |  Battery   | 32           |\n",
      "|   Clothing    | 81           |             |              |            |              |\n",
      "|     total     | 5226         |             |              |            |              |\u001b[0m\n",
      "\u001b[32m[10/11 01:15:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024, sample_style='choice')]\n",
      "\u001b[32m[10/11 01:15:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/11 01:15:24 d2.data.common]: \u001b[0mSerializing 414 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/11 01:15:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.37 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/414 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Evaluating: 100%|██████████| 414/414 [00:30<00:00, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/11 01:15:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[10/11 01:15:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_FPN-based Faster R-CNN/inference/coco_instances_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/11 01:15:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current coco set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py:155\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py:496\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py:310\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:158\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     gt_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mTimmBackbone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     out \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](feature) \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_features.py:346\u001b[0m, in \u001b[0;36mFeatureListNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (List[torch\u001b[38;5;241m.\u001b[39mTensor]):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_features.py:300\u001b[0m, in \u001b[0;36mFeatureDictNet._collect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 243\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal evaluation results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m cfg_fpn \u001b[38;5;241m=\u001b[39m setup_cfg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPN-based Faster R-CNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 243\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_fpn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFPN-based Faster R-CNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# 각 모델 설정 및 학습\u001b[39;00m\n\u001b[1;32m    245\u001b[0m cfg_cascade \u001b[38;5;241m=\u001b[39m setup_cfg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCascade R-CNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 233\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(cfg, model_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m dataset_metadata \u001b[38;5;241m=\u001b[39m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_metadata\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mTqdmTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py:486\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py:165\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mTqdmTrainer.after_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m DetectionCheckpointer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mOUTPUT_DIR)\n\u001b[1;32m     35\u001b[0m checkpointer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mTqdmTrainer.test\u001b[0;34m(self, cfg, model)\u001b[0m\n\u001b[1;32m     49\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_evaluator(cfg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset_name)\n\u001b[1;32m     50\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_test_loader(cfg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset_name)\n\u001b[0;32m---> 51\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43minference_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 입력 데이터를 연속적으로 만들기\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensure_contiguous\u001b[39m(data):\n",
      "Cell \u001b[0;32mIn[3], line 187\u001b[0m, in \u001b[0;36minference_on_dataset\u001b[0;34m(model, data_loader, evaluator, tqdm_desc)\u001b[0m\n\u001b[1;32m    180\u001b[0m total_compute_time_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(total_compute_time)))\n\u001b[1;32m    181\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal inference pure compute time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m s / img per device, on \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m devices)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    183\u001b[0m         total_compute_time_str, total_compute_time \u001b[38;5;241m*\u001b[39m num_devices \u001b[38;5;241m/\u001b[39m total, num_devices\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m )\n\u001b[0;32m--> 187\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# An evaluator may return None when not in main process.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Replace it by an empty dict instead to make it easier for downstream code to handle\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/evaluation/coco_evaluation.py:206\u001b[0m, in \u001b[0;36mCOCOEvaluator.evaluate\u001b[0;34m(self, img_ids)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_box_proposals(predictions)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Copy so the caller can do whatever with results\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/evaluation/coco_evaluation.py:266\u001b[0m, in \u001b[0;36mCOCOEvaluator._eval_predictions\u001b[0;34m(self, predictions, img_ids)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tasks):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     coco_eval \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 266\u001b[0m         \u001b[43m_evaluate_predictions_on_coco\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coco_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoco_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkpt_oks_sigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kpt_oks_sigmas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcocoeval_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOCOeval_opt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_fast_impl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mCOCOeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_dets_per_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_dets_per_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coco_results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# cocoapi does not handle empty results very well\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    279\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_derive_coco_results(\n\u001b[1;32m    280\u001b[0m         coco_eval, task, class_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthing_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results[task] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/evaluation/coco_evaluation.py:590\u001b[0m, in \u001b[0;36m_evaluate_predictions_on_coco\u001b[0;34m(coco_gt, coco_results, iou_type, kpt_oks_sigmas, cocoeval_fn, img_ids, max_dets_per_image)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m coco_results:\n\u001b[1;32m    588\u001b[0m         c\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 590\u001b[0m coco_dt \u001b[38;5;241m=\u001b[39m \u001b[43mcoco_gt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadRes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m coco_eval \u001b[38;5;241m=\u001b[39m cocoeval_fn(coco_gt, coco_dt, iou_type)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# For COCO, the default max_dets_per_image is [1, 10, 100].\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/coco.py:327\u001b[0m, in \u001b[0;36mCOCO.loadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(anns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults in not an array of objects\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    326\u001b[0m annsImgIds \u001b[38;5;241m=\u001b[39m [ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns]\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(annsImgIds) \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mset\u001b[39m(annsImgIds) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetImgIds())), \\\n\u001b[1;32m    328\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults do not correspond to current coco set\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m anns[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    330\u001b[0m     imgIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m([ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns])\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results do not correspond to current coco set"
     ]
    }
   ],
   "source": [
    "class TqdmTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        super().__init__(cfg)\n",
    "        self.pbar = None\n",
    "        self.eval_dataset_name = create_eval_dataset(cfg, cfg.DATASETS.TRAIN[0])\n",
    "\n",
    "    def train(self):\n",
    "        self.pbar = tqdm(total=self.max_iter, desc=\"Training\")\n",
    "        super().train()\n",
    "\n",
    "    def after_step(self):\n",
    "        super().after_step()\n",
    "        self.iter += 1  # iteration 증가\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.update(1)\n",
    "        \n",
    "        # 주기적으로 모델 저장 및 평가 (예: 매 1000 iteration마다)\n",
    "        if self.iter % 1000 == 0:\n",
    "            checkpointer = DetectionCheckpointer(self.model, self.cfg.OUTPUT_DIR)\n",
    "            checkpointer.save(f\"model_{self.iter:07d}\")\n",
    "            \n",
    "            # 평가 데이터로 평가 수행\n",
    "            self.test(self.cfg, self.model)\n",
    "\n",
    "    def after_train(self):\n",
    "        super().after_train()\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.close()\n",
    "        \n",
    "        # 최종 모델 저장 및 평가\n",
    "        checkpointer = DetectionCheckpointer(self.model, self.cfg.OUTPUT_DIR)\n",
    "        checkpointer.save(\"model_final\")\n",
    "        self.test(self.cfg, self.model)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    def build_test_loader(self, cfg, dataset_name):\n",
    "        print(f\"Building test loader for dataset: {dataset_name}\")\n",
    "        return build_detection_test_loader(cfg, dataset_name)\n",
    "\n",
    "    def test(self, cfg, model):\n",
    "        evaluator = self.build_evaluator(cfg, self.eval_dataset_name)\n",
    "        data_loader = self.build_test_loader(cfg, self.eval_dataset_name)\n",
    "        results = inference_on_dataset(model, data_loader, evaluator)\n",
    "        \n",
    "        # 입력 데이터를 연속적으로 만들기\n",
    "        def ensure_contiguous(data):\n",
    "            if isinstance(data, torch.Tensor):\n",
    "                return data.contiguous()\n",
    "            elif isinstance(data, dict):\n",
    "                return {k: ensure_contiguous(v) for k, v in data.items()}\n",
    "            elif isinstance(data, list):\n",
    "                return [ensure_contiguous(v) for v in data]\n",
    "            return data\n",
    "        \n",
    "        contiguous_results = ensure_contiguous(results)\n",
    "        return contiguous_results\n",
    "    \n",
    "def train_model(cfg, model_name):\n",
    "    cfg.OUTPUT_DIR = f\"./output_{model_name}\"\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # 데이터셋 메타데이터 확인\n",
    "    from detectron2.data import MetadataCatalog\n",
    "    dataset_metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    print(\"Dataset structure:\", dataset_metadata)\n",
    "    \n",
    "    trainer = TqdmTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "    print(f\"{model_name} training completed.\")\n",
    "    print(f\"Model saved in {cfg.OUTPUT_DIR}\")\n",
    "    \n",
    "    # 최종 평가 수행\n",
    "    final_results = trainer.test(cfg, trainer.model)\n",
    "    print(f\"Final evaluation results: {final_results}\")\n",
    "\n",
    "def check_category_mapping(predictions, dataset_metadata):\n",
    "    pred_classes = set([pred[\"category_id\"] for pred in predictions])\n",
    "    dataset_classes = set(range(len(dataset_metadata.thing_classes)))\n",
    "    if pred_classes != dataset_classes:\n",
    "        print(f\"Warning: Mismatch in category IDs. Predictions: {pred_classes}, Dataset: {dataset_classes}\")\n",
    "\n",
    "def validate_coco_results(results):\n",
    "    required_keys = [\"image_id\", \"category_id\", \"bbox\", \"score\"]\n",
    "    for result in results:\n",
    "        if not all(key in result for key in required_keys):\n",
    "            print(f\"Invalid result format: {result}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    \"\"\"\n",
    "    A context where the model is temporarily changed to eval mode,\n",
    "    and restored to previous mode afterwards.\n",
    "\n",
    "    Args:\n",
    "        model: a torch Module\n",
    "    \"\"\"\n",
    "    training_mode = model.training\n",
    "    model.eval()\n",
    "    yield\n",
    "    model.train(training_mode)\n",
    "\n",
    "def inference_on_dataset(model, data_loader, evaluator, tqdm_desc=\"Evaluating\"):\n",
    "    \"\"\"\n",
    "    Run model on the data_loader and evaluate the metrics with evaluator.\n",
    "    Also benchmark the inference speed of `model.forward` accurately.\n",
    "    The model will be used in eval mode.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): a module which accepts an object from\n",
    "            `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.\n",
    "\n",
    "            If you wish to evaluate a model in `training` mode instead, you can\n",
    "            wrap the given model and override its behavior of `.eval()` and `.train()`.\n",
    "        data_loader: an iterable object with a length.\n",
    "            The elements it generates will be the inputs to the model.\n",
    "        evaluator (DatasetEvaluator): the evaluator to run. Use `None` if you only want\n",
    "            to benchmark, but don't want to do any evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The return value of `evaluator.evaluate()`\n",
    "    \"\"\"\n",
    "    num_devices = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Start inference on {} images\".format(len(data_loader)))\n",
    "\n",
    "    total = len(data_loader)  # inference data loader must have a fixed length\n",
    "    if evaluator is None:\n",
    "        # create a no-op evaluator\n",
    "        evaluator = DatasetEvaluators([])\n",
    "    evaluator.reset()\n",
    "\n",
    "    num_warmup = min(5, total - 1)\n",
    "    start_time = time.perf_counter()\n",
    "    total_compute_time = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for inputs in tqdm(data_loader, total=total, desc=tqdm_desc):\n",
    "    #         # Ensure inputs are contiguous and on the correct device\n",
    "    #         if isinstance(inputs, list):\n",
    "    #             inputs = [{k: v.contiguous().cuda() if isinstance(v, torch.Tensor) else v \n",
    "    #                        for k, v in d.items()} for d in inputs]\n",
    "    #         elif isinstance(inputs, dict):\n",
    "    #             inputs = {k: v.contiguous().cuda() if isinstance(v, torch.Tensor) else v \n",
    "    #                       for k, v in inputs.items()}\n",
    "\n",
    "    #         outputs = model(inputs)\n",
    "    #         evaluator.process(inputs, outputs)\n",
    "    with inference_context(model), torch.no_grad():\n",
    "        for idx, inputs in enumerate(tqdm(data_loader, total=total, desc=tqdm_desc)):\n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "\n",
    "            start_compute_time = time.perf_counter()\n",
    "            outputs = model(inputs)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            evaluator.process(inputs, outputs)\n",
    "\n",
    "    # Measure the time only for this worker (before the synchronization barrier)\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=total_time))\n",
    "    # NOTE this format is parsed by grep\n",
    "    logger.info(\n",
    "        \"Total inference time: {} ({:.6f} s / img per device, on {} devices)\".format(\n",
    "            total_time_str, total_time * num_devices / total, num_devices\n",
    "        )\n",
    "    )\n",
    "    total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))\n",
    "    logger.info(\n",
    "        \"Total inference pure compute time: {} ({:.6f} s / img per device, on {} devices)\".format(\n",
    "            total_compute_time_str, total_compute_time * num_devices / total, num_devices\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = evaluator.evaluate()\n",
    "    # An evaluator may return None when not in main process.\n",
    "    # Replace it by an empty dict instead to make it easier for downstream code to handle\n",
    "    if results is None:\n",
    "        results = {}\n",
    "    return results\n",
    "\n",
    "def create_eval_dataset(cfg, dataset_name, percentage=0.02):\n",
    "    # 전체 데이터셋 로드\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    \n",
    "    # 클래스별로 데이터 분류\n",
    "    class_data = {}\n",
    "    for data in dataset_dicts:\n",
    "        for ann in data['annotations']:\n",
    "            category_id = ann['category_id']\n",
    "            if category_id not in class_data:\n",
    "                class_data[category_id] = []\n",
    "            class_data[category_id].append(data)\n",
    "    \n",
    "    # 각 클래스에서 20% 샘플링\n",
    "    eval_data = []\n",
    "    for category, items in class_data.items():\n",
    "        sample_size = max(1, int(len(items) * percentage))\n",
    "        eval_data.extend(random.sample(items, sample_size))\n",
    "    \n",
    "    # 중복 제거\n",
    "    eval_data = list({d['image_id']: d for d in eval_data}.values())\n",
    "    \n",
    "    # 평가 데이터셋 등록\n",
    "    eval_dataset_name = f\"{dataset_name}_eval\"\n",
    "    DatasetCatalog.register(eval_dataset_name, lambda: eval_data)\n",
    "    MetadataCatalog.get(eval_dataset_name).set(thing_classes=MetadataCatalog.get(dataset_name).thing_classes)\n",
    "    \n",
    "    return eval_dataset_name\n",
    "def train_model(cfg, model_name):\n",
    "    cfg.OUTPUT_DIR = f\"./output_{model_name}\"\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    trainer = TqdmTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    # 데이터셋 메타데이터 확인\n",
    "    dataset_metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    print(f\"Dataset version: {dataset_metadata.version}\")\n",
    "    \n",
    "    trainer.train()\n",
    "    print(f\"{model_name} training completed.\")\n",
    "    print(f\"Model saved in {cfg.OUTPUT_DIR}\")\n",
    "    \n",
    "    # 최종 평가 수행\n",
    "    final_results = trainer.test(cfg, trainer.model)\n",
    "    print(f\"Final evaluation results: {final_results}\")\n",
    "\n",
    "\n",
    "cfg_fpn = setup_cfg(\"FPN-based Faster R-CNN\")\n",
    "train_model(cfg_fpn, \"FPN-based Faster R-CNN\")\n",
    "# 각 모델 설정 및 학습\n",
    "cfg_cascade = setup_cfg(\"Cascade R-CNN\")\n",
    "train_model(cfg_cascade, \"Cascade R-CNN\")\n",
    "\n",
    "cfg_retinanet = setup_cfg(\"RetinaNet\")\n",
    "train_model(cfg_retinanet, \"RetinaNet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
